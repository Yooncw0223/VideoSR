{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN52AIsVu15b",
        "outputId": "002377ee-5658-4263-edf9-98ed1fff689d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# I first mount my google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtHBbiwWxeM-",
        "outputId": "720cc8e3-e669-4328-d646-5f0337e8f6ea"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/6.8300 Final Project/cvproject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I then manage path to make path-dependent commands simpler\n",
        "import os\n",
        "path = \"/content/drive/MyDrive/6.8300 Final Project/cvproject/\"\n",
        "\n",
        "\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exq5DHT_vCjH",
        "outputId": "29ab88b4-dac1-46f9-efd5-1c8121ebf2d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['val',\n",
              " 'Copy of hello.avi',\n",
              " '.ipynb_checkpoints',\n",
              " 'network_swinir.py',\n",
              " 'train_sharp.zip',\n",
              " 'train_sharp_bicubic.zip',\n",
              " 'uc?id=1a4PrjqT-hShvY9IyJm3sPF0ZaXyrCozR',\n",
              " 'util_calculate_psnr_ssim.py',\n",
              " 'val_sharp.zip',\n",
              " 'val_sharp_bicubic.zip',\n",
              " 'video_dataset.py',\n",
              " 'vimeo_super_resolution_test.zip',\n",
              " 'train',\n",
              " 'train_blurry',\n",
              " 'val_blurry',\n",
              " '__pycache__',\n",
              " 'SwinIR',\n",
              " 'hello.avi',\n",
              " 'imageSRModel.pth',\n",
              " 'vanilla_sr_new.avi']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow\n",
        "!pip install -U image\n",
        "!pip install opencv-python\n",
        "!pip install tqdm\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "\n",
        "from tqdm import tqdm\n",
        "from io import BytesIO\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from IPython.display import Image, clear_output, display\n",
        "\n",
        "# PyTorch will be out main tool for playing with neural networks\n",
        "import torch\n",
        "import torch.hub\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, datasets, transforms\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# CPU / GPU\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPXr-RdQ-DZj",
        "outputId": "0fdbda0f-7cd2-4d57-baa5-37844129d2f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (8.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting image\n",
            "  Downloading image-1.5.33.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from image) (8.4.0)\n",
            "Collecting django (from image)\n",
            "  Downloading Django-4.2.1-py3-none-any.whl (8.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from image) (1.16.0)\n",
            "Collecting asgiref<4,>=3.6.0 (from django->image)\n",
            "  Downloading asgiref-3.6.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from django->image) (0.4.4)\n",
            "Building wheels for collected packages: image\n",
            "  Building wheel for image (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for image: filename=image-1.5.33-py2.py3-none-any.whl size=19483 sha256=d42bef6baa104270d80da777c2351ef9c3fd3c0bab17bce418834eeccf8e3e19\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/0c/a4/7cfa53a5c6225c2db2bfec08e782b43d0f25fdae2e995b69be\n",
            "Successfully built image\n",
            "Installing collected packages: asgiref, django, image\n",
            "Successfully installed asgiref-3.6.0 django-4.2.1 image-1.5.33\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.0+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (16.0.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss Functions and Metrics"
      ],
      "metadata": {
        "id": "vYmOgr-pwbP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "U9xjvb1WvPMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf52aa7-1786-43ee-b8d4-ef35dd8ac68b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Datasets"
      ],
      "metadata": {
        "id": "KmQgqRoxxdlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we use a library by RaivoKoot to make the video dataset easier\n",
        "# !git clone https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch.git\n",
        "\n",
        "# and move video_dataset.py to the main project folder to avoid hassle with import in python!\n",
        "\n",
        "# now let's generate annotation.txt that is necessary for the library to function\n",
        "\n",
        "def format_seq_num(number, total_digits):\n",
        "    return (\"0\" * (total_digits - len(str(number)))) + str(number)\n",
        "\n",
        "num_samples = [240, 30]\n",
        "name_folder = [\"train\", \"val\"]\n",
        "\n",
        "# for sharp/GT videos\n",
        "annotation_content = []\n",
        "for num_sample, name in zip(num_samples, name_folder):\n",
        "    sharp_annotation_content = []\n",
        "    for i in range(num_sample):\n",
        "        row = format_seq_num(i, 3) + \" 0 99 \" + str(i) + \"\\n\"\n",
        "        sharp_annotation_content.append(row)\n",
        "    annotation_content.append(sharp_annotation_content)\n",
        "\n",
        "with open(\"train/annotation.txt\", \"w\") as annotation:\n",
        "    annotation.writelines(annotation_content[0])\n",
        "\n",
        "with open(\"train_blurry/train_sharp_bicubic/annotation.txt\", \"w\") as annotation:\n",
        "    annotation.writelines(annotation_content[0])\n",
        "\n",
        "with open(\"val/annotation.txt\", \"w\") as annotation:\n",
        "    annotation.writelines(annotation_content[1])\n",
        "\n",
        "with open(\"val_blurry/val_sharp_bicubic/annotation.txt\", \"w\") as annotation:\n",
        "    annotation.writelines(annotation_content[1])"
      ],
      "metadata": {
        "id": "zkkCs-D0_o9x"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import first before creating dataset\n",
        "from video_dataset import VideoFrameDataset, ImglistToTensor\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import os\n",
        "\n",
        "import tqdm\n",
        "import matplotlib\n",
        "from matplotlib import animation, rc\n",
        "\n",
        "import cv2\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "nrs1nzYcAtET"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "NUM_FRAMES = 100"
      ],
      "metadata": {
        "id": "-xKhrtNLcjA_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharp_train_root = os.path.join(path, \"train\")\n",
        "sharp_train_annotation_file = os.path.join(sharp_train_root, 'annotation.txt')\n",
        "sharp_train_root = os.path.join(sharp_train_root, \"train_sharp\")\n",
        "\n",
        "blurry_train_root = os.path.join(path, \"train_blurry\", \"train_sharp_bicubic\")\n",
        "blurry_train_annotation_file = os.path.join(blurry_train_root, \"annotation.txt\")\n",
        "blurry_train_root = os.path.join(blurry_train_root, \"X4\")\n",
        "\n",
        "sharp_val_root = os.path.join(path, \"val\")\n",
        "sharp_val_annotation_file = os.path.join(sharp_val_root, 'annotation.txt')\n",
        "sharp_val_root = os.path.join(sharp_val_root, \"val_sharp\")\n",
        "\n",
        "blurry_val_root = os.path.join(path, \"val_blurry\", \"val_sharp_bicubic\")\n",
        "blurry_val_annotation_file = os.path.join(blurry_val_root, \"annotation.txt\")\n",
        "blurry_val_root = os.path.join(blurry_val_root, \"X4\")"
      ],
      "metadata": {
        "id": "PxLoI3nrxgJW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([\n",
        "    ImglistToTensor(),  # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # standard normalization\n",
        "])\n",
        "\n",
        "def denormalize(video_tensor):\n",
        "    \"\"\"\n",
        "    Undoes mean/standard deviation normalization, zero to one scaling,\n",
        "    and channel rearrangement for a batch of images.\n",
        "    args:\n",
        "        video_tensor: a (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
        "    \"\"\"\n",
        "    inverse_normalize = transforms.Normalize(\n",
        "        mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
        "        std=[1 / 0.229, 1 / 0.224, 1 / 0.225]\n",
        "    )\n",
        "    return (inverse_normalize(video_tensor) * 255.).type(torch.uint8).permute(0, 2, 3, 1).numpy()"
      ],
      "metadata": {
        "id": "2-zg-aqT88bk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# might want to lower the resolution for better speed\n",
        "\n",
        "sharp_train_dataset = VideoFrameDataset(\n",
        "    root_path = sharp_train_root,\n",
        "    annotationfile_path=sharp_train_annotation_file,\n",
        "    num_segments=NUM_FRAMES,\n",
        "    frames_per_segment=1,\n",
        "    imagefile_template=\"{:08d}.png\",\n",
        "    transform=preprocess,\n",
        "    test_mode=False\n",
        ")\n",
        "\n",
        "sample = sharp_train_dataset[3]\n",
        "frames = sample[0]  # list of PIL images\n",
        "label = sample[1]  # integer label"
      ],
      "metadata": {
        "id": "8Nnx-GKr8xVU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# might want to lower the resolution for better speed\n",
        "\n",
        "blurry_train_dataset = VideoFrameDataset(\n",
        "    root_path = blurry_train_root,\n",
        "    annotationfile_path=blurry_train_annotation_file,\n",
        "    num_segments=NUM_FRAMES,\n",
        "    frames_per_segment=1,\n",
        "    imagefile_template=\"{:08d}.png\",\n",
        "    transform=preprocess,\n",
        "    test_mode=False\n",
        ")\n",
        "\n",
        "sample = blurry_train_dataset[3]\n",
        "frames = sample[0]  # list of PIL images\n",
        "label = sample[1]  # integer label"
      ],
      "metadata": {
        "id": "SvkbZle1wumJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# might want to lower the resolution for better speed\n",
        "\n",
        "sharp_val_dataset = VideoFrameDataset(\n",
        "    root_path = sharp_val_root,\n",
        "    annotationfile_path=sharp_val_annotation_file,\n",
        "    num_segments=NUM_FRAMES,\n",
        "    frames_per_segment=1,\n",
        "    imagefile_template=\"{:08d}.png\",\n",
        "    transform=preprocess,\n",
        "    test_mode=False\n",
        ")\n",
        "\n",
        "sample = sharp_val_dataset[3]\n",
        "frames = sample[0]  # list of PIL images\n",
        "label = sample[1]  # integer label"
      ],
      "metadata": {
        "id": "5GIBEFaAyz_D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# might want to lower the resolution for better speed\n",
        "\n",
        "blurry_val_dataset = VideoFrameDataset(\n",
        "    root_path = blurry_val_root,\n",
        "    annotationfile_path=blurry_val_annotation_file,\n",
        "    num_segments=NUM_FRAMES,\n",
        "    frames_per_segment=1,\n",
        "    imagefile_template=\"{:08d}.png\",\n",
        "    transform=preprocess,\n",
        "    test_mode=False\n",
        ")\n",
        "\n",
        "sample = blurry_val_dataset[3]\n",
        "frames = sample[0]  # list of PIL images\n",
        "label = sample[1]  # integer label"
      ],
      "metadata": {
        "id": "6GlUoNqZ0DkZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert video from BGR to RGB\n",
        "\n",
        "def convertVideoBGRtoRGB(video):\n",
        "    return video[:, :, :, [2, 1, 0]]\n",
        "\n",
        "def get_video_output(normalized_video):\n",
        "    return convertVideoBGRtoRGB(denormalize(normalized_video))"
      ],
      "metadata": {
        "id": "qi6IVJX8-sqv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# frames = sample[0]\n",
        "# # frames = [cv2.cvtColor(np.array(frame), cv2.COLOR_BGR2RGB) for frame in frames]\n",
        "# # frames = np.array(frames)\n",
        "# frames = denormalize(frames)\n",
        "# frames = convertVideoBGRtoRGB(frames)\n",
        "\n",
        "# print(frames.shape)\n",
        "\n",
        "\n",
        "# # test_video = AnimationWrapper(rows=1, cols=1, frames=frames)\n",
        "# # test_video.generate()\n",
        "# # test_video.anim\n",
        "\n",
        "# _, height, width, layers = frames.shape\n",
        "\n",
        "# video = cv2.VideoWriter(\"hello.avi\", 0, 24, (width,height))\n",
        "\n",
        "# for image in frames:\n",
        "#     video.write(image)\n",
        "# video.release()\n",
        "# cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "JkRqUwbpeRQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e722478-42c7-48a0-d1ad-98804e440c03"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 720, 1280, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SwinIR: Image Superresolution Model"
      ],
      "metadata": {
        "id": "hHTE8wpZC48A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/JingyunLiang/SwinIR.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRgqhwTZFMWj",
        "outputId": "8175c7e9-4b34-4676-d180-a2af3bdf3073"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'SwinIR' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6FKhHFAIWaf",
        "outputId": "095602cc-8399-43c0-d16a-9d18944911ca"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.1+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.14.1 timm-0.6.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "import torch\n",
        "import requests\n",
        "\n",
        "from network_swinir import SwinIR as net\n",
        "import util_calculate_psnr_ssim as util"
      ],
      "metadata": {
        "id": "sxi5FhPCFqzN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMAGE SR model-specific parameter\n",
        "SCALE_FACTOR = 4\n",
        "MODEL_PATH = \"model_zoo/swinir/002_lightweightSR_DIV2K_s64w8_SwinIR-S_x4.pth\"\n",
        "ARG_TASK = \"lightweight_sr\"\n",
        "WINDOW_SIZE = 8\n",
        "BORDER = SCALE_FACTOR\n",
        "# python main_test_swinir.py --task lightweight_sr --scale 4 \n",
        "# --model_path model_zoo/swinir/002_lightweightSR_DIV2K_s64w8_SwinIR-S_x4.pth\n",
        "# --folder_lq testsets/Set5/LR_bicubic/X4 --folder_gt testsets/Set5/HR\n"
      ],
      "metadata": {
        "id": "m1pNSUJOJNll"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# set up model\n",
        "url = 'https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/{}'.format(os.path.basename(MODEL_PATH))\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "print(f'downloading model')\n",
        "open(os.path.join(os.getcwd(), \"imageSRModel.pth\"), 'wb').write(r.content)\n",
        "\n",
        "model = net(upscale=SCALE_FACTOR, in_chans=3, img_size=64, window_size=8,\n",
        "                    img_range=1., depths=[6, 6, 6, 6], embed_dim=60, num_heads=[6, 6, 6, 6],\n",
        "                    mlp_ratio=2, upsampler='pixelshuffledirect', resi_connection='1conv')\n",
        "param_key_g = 'params'\n",
        "\n",
        "pretrained_model = torch.load(\"imageSRModel.pth\")\n",
        "model.load_state_dict(pretrained_model[param_key_g] if param_key_g in pretrained_model.keys() else pretrained_model, strict=True)\n",
        "\n",
        "model.eval()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvqJkrQNC4Ud",
        "outputId": "5b75fece-ca58-42b9-deb8-98729a1dd9e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = OrderedDict()\n",
        "test_results['psnr'] = []\n",
        "test_results['ssim'] = []\n",
        "test_results['psnr_y'] = []\n",
        "test_results['ssim_y'] = []\n",
        "test_results['psnrb'] = []\n",
        "test_results['psnrb_y'] = []\n",
        "psnr, ssim, psnr_y, ssim_y, psnrb, psnrb_y = 0, 0, 0, 0, 0, 0\n",
        "\n",
        "vanilla_sr_video = []\n",
        "lr_video = blurry_train_dataset[3][0] # 3rd data point in the blurry training version\n",
        "lr_video = get_video_output(lr_video)\n",
        "# lr_video = [cv2.cvtColor(np.array(frame), cv2.COLOR_BGR2RGB) for frame in lr_video]\n",
        "gt_video = sharp_train_dataset[3][0] # 3rd data point in the sharp training version\n",
        "gt_video = get_video_output(gt_video)\n",
        "# gt_video = [cv2.cvtColor(np.array(frame), cv2.COLOR_BGR2RGB) for frame in gt_video]\n",
        "\n",
        "lr_video = [frame / 255 for frame in lr_video]\n",
        "gt_video = [frame / 255 for frame in gt_video]\n",
        "\n",
        "for idx, (img_lq, img_gt) in enumerate(zip(lr_video, gt_video)):\n",
        "    # read image\n",
        "    img_lq = np.transpose(img_lq if img_lq.shape[2] == 1 else img_lq[:, :, [2, 1, 0]], (2, 0, 1))  # HCW-BGR to CHW-RGB\n",
        "    img_lq = torch.from_numpy(img_lq).float().unsqueeze(0).to(device)  # CHW-RGB to NCHW-RGB\n",
        "\n",
        "    # inference\n",
        "    with torch.no_grad():\n",
        "        # pad input image to be a multiple of window_size\n",
        "        _, _, h_old, w_old = img_lq.size()\n",
        "        h_pad = (h_old // WINDOW_SIZE + 1) * WINDOW_SIZE - h_old\n",
        "        w_pad = (w_old // WINDOW_SIZE + 1) * WINDOW_SIZE - w_old\n",
        "        img_lq = torch.cat([img_lq, torch.flip(img_lq, [2])], 2)[:, :, :h_old + h_pad, :]\n",
        "        img_lq = torch.cat([img_lq, torch.flip(img_lq, [3])], 3)[:, :, :, :w_old + w_pad]\n",
        "        output = model(img_lq)\n",
        "        output = output[..., :h_old * SCALE_FACTOR, :w_old * SCALE_FACTOR]\n",
        "\n",
        "    # save image\n",
        "    output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
        "    if output.ndim == 3:\n",
        "        output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))  # CHW-RGB to HCW-BGR\n",
        "    output = (output * 255.0).round().astype(np.uint8)  # float32 to uint8\n",
        "    \n",
        "    # ----------------------------------------------------\n",
        "    # appending to later make a video\n",
        "    vanilla_sr_video.append(output)\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "\n",
        "    # evaluate psnr/ssim/psnr_b\n",
        "    if img_gt is not None:\n",
        "        img_gt = (img_gt * 255.0).round().astype(np.uint8)  # float32 to uint8\n",
        "        img_gt = img_gt[:h_old * SCALE_FACTOR, :w_old * SCALE_FACTOR, ...]  # crop gt\n",
        "        img_gt = np.squeeze(img_gt)\n",
        "\n",
        "        psnr = util.calculate_psnr(output, img_gt, crop_border=BORDER)\n",
        "        ssim = util.calculate_ssim(output, img_gt, crop_border=BORDER)\n",
        "        test_results['psnr'].append(psnr)\n",
        "        test_results['ssim'].append(ssim)\n",
        "        if img_gt.ndim == 3:  # RGB image\n",
        "            psnr_y = util.calculate_psnr(output, img_gt, crop_border=BORDER, test_y_channel=True)\n",
        "            ssim_y = util.calculate_ssim(output, img_gt, crop_border=BORDER, test_y_channel=True)\n",
        "            test_results['psnr_y'].append(psnr_y)\n",
        "            test_results['ssim_y'].append(ssim_y)\n",
        "        if ARG_TASK in ['jpeg_car', 'color_jpeg_car']:\n",
        "            psnrb = util.calculate_psnrb(output, img_gt, crop_border=BORDER, test_y_channel=False)\n",
        "            test_results['psnrb'].append(psnrb)\n",
        "            if ARG_TASK in ['color_jpeg_car']:\n",
        "                psnrb_y = util.calculate_psnrb(output, img_gt, crop_border=BORDER, test_y_channel=True)\n",
        "                test_results['psnrb_y'].append(psnrb_y)\n",
        "        print('Testing {:d} {:20s} - PSNR: {:.2f} dB; SSIM: {:.4f}; PSNRB: {:.2f} dB;'\n",
        "                'PSNR_Y: {:.2f} dB; SSIM_Y: {:.4f}; PSNRB_Y: {:.2f} dB.'.\n",
        "                format(idx, \"imgname\", psnr, ssim, psnrb, psnr_y, ssim_y, psnrb_y))\n",
        "    else:\n",
        "        print('Testing {:d} {:20s}'.format(idx, \"imgname\"))\n",
        "\n",
        "# summarize psnr/ssim\n",
        "if img_gt is not None:\n",
        "    ave_psnr = sum(test_results['psnr']) / len(test_results['psnr'])\n",
        "    ave_ssim = sum(test_results['ssim']) / len(test_results['ssim'])\n",
        "    print('\\n-- Average PSNR/SSIM(RGB): {:.2f} dB; {:.4f}'.format(ave_psnr, ave_ssim))\n",
        "    if img_gt.ndim == 3:\n",
        "        ave_psnr_y = sum(test_results['psnr_y']) / len(test_results['psnr_y'])\n",
        "        ave_ssim_y = sum(test_results['ssim_y']) / len(test_results['ssim_y'])\n",
        "        print('-- Average PSNR_Y/SSIM_Y: {:.2f} dB; {:.4f}'.format(ave_psnr_y, ave_ssim_y))\n",
        "    if ARG_TASK in ['jpeg_car', 'color_jpeg_car']:\n",
        "        ave_psnrb = sum(test_results['psnrb']) / len(test_results['psnrb'])\n",
        "        print('-- Average PSNRB: {:.2f} dB'.format(ave_psnrb))\n",
        "        if ARG_TASK in ['color_jpeg_car']:\n",
        "            ave_psnrb_y = sum(test_results['psnrb_y']) / len(test_results['psnrb_y'])\n",
        "            print('-- Average PSNRB_Y: {:.2f} dB'.format(ave_psnrb_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB6x67tNQevv",
        "outputId": "9020faf2-c465-4fc6-bfe0-be08610dd7a9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing 0 imgname              - PSNR: 27.66 dB; SSIM: 0.7275; PSNRB: 0.00 dB;PSNR_Y: 29.02 dB; SSIM_Y: 0.7539; PSNRB_Y: 0.00 dB.\n",
            "Testing 1 imgname              - PSNR: 27.48 dB; SSIM: 0.7242; PSNRB: 0.00 dB;PSNR_Y: 28.83 dB; SSIM_Y: 0.7502; PSNRB_Y: 0.00 dB.\n",
            "Testing 2 imgname              - PSNR: 27.48 dB; SSIM: 0.7216; PSNRB: 0.00 dB;PSNR_Y: 28.84 dB; SSIM_Y: 0.7476; PSNRB_Y: 0.00 dB.\n",
            "Testing 3 imgname              - PSNR: 27.45 dB; SSIM: 0.7202; PSNRB: 0.00 dB;PSNR_Y: 28.81 dB; SSIM_Y: 0.7465; PSNRB_Y: 0.00 dB.\n",
            "Testing 4 imgname              - PSNR: 27.57 dB; SSIM: 0.7252; PSNRB: 0.00 dB;PSNR_Y: 28.93 dB; SSIM_Y: 0.7513; PSNRB_Y: 0.00 dB.\n",
            "Testing 5 imgname              - PSNR: 27.52 dB; SSIM: 0.7237; PSNRB: 0.00 dB;PSNR_Y: 28.87 dB; SSIM_Y: 0.7499; PSNRB_Y: 0.00 dB.\n",
            "Testing 6 imgname              - PSNR: 27.56 dB; SSIM: 0.7319; PSNRB: 0.00 dB;PSNR_Y: 28.91 dB; SSIM_Y: 0.7575; PSNRB_Y: 0.00 dB.\n",
            "Testing 7 imgname              - PSNR: 27.46 dB; SSIM: 0.7323; PSNRB: 0.00 dB;PSNR_Y: 28.80 dB; SSIM_Y: 0.7574; PSNRB_Y: 0.00 dB.\n",
            "Testing 8 imgname              - PSNR: 27.50 dB; SSIM: 0.7297; PSNRB: 0.00 dB;PSNR_Y: 28.86 dB; SSIM_Y: 0.7554; PSNRB_Y: 0.00 dB.\n",
            "Testing 9 imgname              - PSNR: 27.64 dB; SSIM: 0.7336; PSNRB: 0.00 dB;PSNR_Y: 29.00 dB; SSIM_Y: 0.7592; PSNRB_Y: 0.00 dB.\n",
            "Testing 10 imgname              - PSNR: 27.63 dB; SSIM: 0.7328; PSNRB: 0.00 dB;PSNR_Y: 28.99 dB; SSIM_Y: 0.7582; PSNRB_Y: 0.00 dB.\n",
            "Testing 11 imgname              - PSNR: 27.54 dB; SSIM: 0.7282; PSNRB: 0.00 dB;PSNR_Y: 28.90 dB; SSIM_Y: 0.7543; PSNRB_Y: 0.00 dB.\n",
            "Testing 12 imgname              - PSNR: 27.56 dB; SSIM: 0.7275; PSNRB: 0.00 dB;PSNR_Y: 28.92 dB; SSIM_Y: 0.7540; PSNRB_Y: 0.00 dB.\n",
            "Testing 13 imgname              - PSNR: 27.51 dB; SSIM: 0.7296; PSNRB: 0.00 dB;PSNR_Y: 28.86 dB; SSIM_Y: 0.7557; PSNRB_Y: 0.00 dB.\n",
            "Testing 14 imgname              - PSNR: 27.43 dB; SSIM: 0.7322; PSNRB: 0.00 dB;PSNR_Y: 28.79 dB; SSIM_Y: 0.7580; PSNRB_Y: 0.00 dB.\n",
            "Testing 15 imgname              - PSNR: 27.32 dB; SSIM: 0.7270; PSNRB: 0.00 dB;PSNR_Y: 28.67 dB; SSIM_Y: 0.7530; PSNRB_Y: 0.00 dB.\n",
            "Testing 16 imgname              - PSNR: 27.27 dB; SSIM: 0.7243; PSNRB: 0.00 dB;PSNR_Y: 28.63 dB; SSIM_Y: 0.7504; PSNRB_Y: 0.00 dB.\n",
            "Testing 17 imgname              - PSNR: 27.21 dB; SSIM: 0.7239; PSNRB: 0.00 dB;PSNR_Y: 28.57 dB; SSIM_Y: 0.7499; PSNRB_Y: 0.00 dB.\n",
            "Testing 18 imgname              - PSNR: 27.17 dB; SSIM: 0.7225; PSNRB: 0.00 dB;PSNR_Y: 28.53 dB; SSIM_Y: 0.7489; PSNRB_Y: 0.00 dB.\n",
            "Testing 19 imgname              - PSNR: 26.99 dB; SSIM: 0.7175; PSNRB: 0.00 dB;PSNR_Y: 28.34 dB; SSIM_Y: 0.7438; PSNRB_Y: 0.00 dB.\n",
            "Testing 20 imgname              - PSNR: 26.99 dB; SSIM: 0.7177; PSNRB: 0.00 dB;PSNR_Y: 28.34 dB; SSIM_Y: 0.7436; PSNRB_Y: 0.00 dB.\n",
            "Testing 21 imgname              - PSNR: 26.97 dB; SSIM: 0.7235; PSNRB: 0.00 dB;PSNR_Y: 28.33 dB; SSIM_Y: 0.7488; PSNRB_Y: 0.00 dB.\n",
            "Testing 22 imgname              - PSNR: 27.08 dB; SSIM: 0.7337; PSNRB: 0.00 dB;PSNR_Y: 28.44 dB; SSIM_Y: 0.7581; PSNRB_Y: 0.00 dB.\n",
            "Testing 23 imgname              - PSNR: 27.09 dB; SSIM: 0.7389; PSNRB: 0.00 dB;PSNR_Y: 28.45 dB; SSIM_Y: 0.7629; PSNRB_Y: 0.00 dB.\n",
            "Testing 24 imgname              - PSNR: 26.87 dB; SSIM: 0.7336; PSNRB: 0.00 dB;PSNR_Y: 28.22 dB; SSIM_Y: 0.7581; PSNRB_Y: 0.00 dB.\n",
            "Testing 25 imgname              - PSNR: 26.98 dB; SSIM: 0.7328; PSNRB: 0.00 dB;PSNR_Y: 28.33 dB; SSIM_Y: 0.7572; PSNRB_Y: 0.00 dB.\n",
            "Testing 26 imgname              - PSNR: 27.05 dB; SSIM: 0.7375; PSNRB: 0.00 dB;PSNR_Y: 28.40 dB; SSIM_Y: 0.7614; PSNRB_Y: 0.00 dB.\n",
            "Testing 27 imgname              - PSNR: 27.08 dB; SSIM: 0.7429; PSNRB: 0.00 dB;PSNR_Y: 28.43 dB; SSIM_Y: 0.7666; PSNRB_Y: 0.00 dB.\n",
            "Testing 28 imgname              - PSNR: 27.21 dB; SSIM: 0.7540; PSNRB: 0.00 dB;PSNR_Y: 28.57 dB; SSIM_Y: 0.7769; PSNRB_Y: 0.00 dB.\n",
            "Testing 29 imgname              - PSNR: 27.36 dB; SSIM: 0.7600; PSNRB: 0.00 dB;PSNR_Y: 28.72 dB; SSIM_Y: 0.7825; PSNRB_Y: 0.00 dB.\n",
            "Testing 30 imgname              - PSNR: 27.50 dB; SSIM: 0.7641; PSNRB: 0.00 dB;PSNR_Y: 28.85 dB; SSIM_Y: 0.7864; PSNRB_Y: 0.00 dB.\n",
            "Testing 31 imgname              - PSNR: 27.41 dB; SSIM: 0.7627; PSNRB: 0.00 dB;PSNR_Y: 28.77 dB; SSIM_Y: 0.7845; PSNRB_Y: 0.00 dB.\n",
            "Testing 32 imgname              - PSNR: 27.34 dB; SSIM: 0.7614; PSNRB: 0.00 dB;PSNR_Y: 28.70 dB; SSIM_Y: 0.7833; PSNRB_Y: 0.00 dB.\n",
            "Testing 33 imgname              - PSNR: 27.18 dB; SSIM: 0.7577; PSNRB: 0.00 dB;PSNR_Y: 28.54 dB; SSIM_Y: 0.7797; PSNRB_Y: 0.00 dB.\n",
            "Testing 34 imgname              - PSNR: 27.15 dB; SSIM: 0.7566; PSNRB: 0.00 dB;PSNR_Y: 28.51 dB; SSIM_Y: 0.7789; PSNRB_Y: 0.00 dB.\n",
            "Testing 35 imgname              - PSNR: 27.28 dB; SSIM: 0.7633; PSNRB: 0.00 dB;PSNR_Y: 28.64 dB; SSIM_Y: 0.7853; PSNRB_Y: 0.00 dB.\n",
            "Testing 36 imgname              - PSNR: 27.49 dB; SSIM: 0.7744; PSNRB: 0.00 dB;PSNR_Y: 28.84 dB; SSIM_Y: 0.7956; PSNRB_Y: 0.00 dB.\n",
            "Testing 37 imgname              - PSNR: 27.53 dB; SSIM: 0.7765; PSNRB: 0.00 dB;PSNR_Y: 28.89 dB; SSIM_Y: 0.7974; PSNRB_Y: 0.00 dB.\n",
            "Testing 38 imgname              - PSNR: 27.62 dB; SSIM: 0.7803; PSNRB: 0.00 dB;PSNR_Y: 28.98 dB; SSIM_Y: 0.8008; PSNRB_Y: 0.00 dB.\n",
            "Testing 39 imgname              - PSNR: 27.69 dB; SSIM: 0.7831; PSNRB: 0.00 dB;PSNR_Y: 29.05 dB; SSIM_Y: 0.8037; PSNRB_Y: 0.00 dB.\n",
            "Testing 40 imgname              - PSNR: 27.70 dB; SSIM: 0.7814; PSNRB: 0.00 dB;PSNR_Y: 29.07 dB; SSIM_Y: 0.8023; PSNRB_Y: 0.00 dB.\n",
            "Testing 41 imgname              - PSNR: 27.80 dB; SSIM: 0.7870; PSNRB: 0.00 dB;PSNR_Y: 29.16 dB; SSIM_Y: 0.8075; PSNRB_Y: 0.00 dB.\n",
            "Testing 42 imgname              - PSNR: 27.61 dB; SSIM: 0.7890; PSNRB: 0.00 dB;PSNR_Y: 28.97 dB; SSIM_Y: 0.8088; PSNRB_Y: 0.00 dB.\n",
            "Testing 43 imgname              - PSNR: 27.54 dB; SSIM: 0.7849; PSNRB: 0.00 dB;PSNR_Y: 28.90 dB; SSIM_Y: 0.8048; PSNRB_Y: 0.00 dB.\n",
            "Testing 44 imgname              - PSNR: 27.62 dB; SSIM: 0.7795; PSNRB: 0.00 dB;PSNR_Y: 28.98 dB; SSIM_Y: 0.7998; PSNRB_Y: 0.00 dB.\n",
            "Testing 45 imgname              - PSNR: 27.66 dB; SSIM: 0.7755; PSNRB: 0.00 dB;PSNR_Y: 29.03 dB; SSIM_Y: 0.7964; PSNRB_Y: 0.00 dB.\n",
            "Testing 46 imgname              - PSNR: 27.73 dB; SSIM: 0.7708; PSNRB: 0.00 dB;PSNR_Y: 29.09 dB; SSIM_Y: 0.7924; PSNRB_Y: 0.00 dB.\n",
            "Testing 47 imgname              - PSNR: 27.63 dB; SSIM: 0.7669; PSNRB: 0.00 dB;PSNR_Y: 28.99 dB; SSIM_Y: 0.7886; PSNRB_Y: 0.00 dB.\n",
            "Testing 48 imgname              - PSNR: 27.70 dB; SSIM: 0.7703; PSNRB: 0.00 dB;PSNR_Y: 29.06 dB; SSIM_Y: 0.7918; PSNRB_Y: 0.00 dB.\n",
            "Testing 49 imgname              - PSNR: 27.70 dB; SSIM: 0.7748; PSNRB: 0.00 dB;PSNR_Y: 29.07 dB; SSIM_Y: 0.7960; PSNRB_Y: 0.00 dB.\n",
            "Testing 50 imgname              - PSNR: 27.97 dB; SSIM: 0.7873; PSNRB: 0.00 dB;PSNR_Y: 29.33 dB; SSIM_Y: 0.8078; PSNRB_Y: 0.00 dB.\n",
            "Testing 51 imgname              - PSNR: 27.75 dB; SSIM: 0.7779; PSNRB: 0.00 dB;PSNR_Y: 29.10 dB; SSIM_Y: 0.7989; PSNRB_Y: 0.00 dB.\n",
            "Testing 52 imgname              - PSNR: 27.60 dB; SSIM: 0.7742; PSNRB: 0.00 dB;PSNR_Y: 28.96 dB; SSIM_Y: 0.7954; PSNRB_Y: 0.00 dB.\n",
            "Testing 53 imgname              - PSNR: 27.65 dB; SSIM: 0.7764; PSNRB: 0.00 dB;PSNR_Y: 29.00 dB; SSIM_Y: 0.7976; PSNRB_Y: 0.00 dB.\n",
            "Testing 54 imgname              - PSNR: 27.69 dB; SSIM: 0.7756; PSNRB: 0.00 dB;PSNR_Y: 29.05 dB; SSIM_Y: 0.7974; PSNRB_Y: 0.00 dB.\n",
            "Testing 55 imgname              - PSNR: 27.61 dB; SSIM: 0.7749; PSNRB: 0.00 dB;PSNR_Y: 28.97 dB; SSIM_Y: 0.7966; PSNRB_Y: 0.00 dB.\n",
            "Testing 56 imgname              - PSNR: 27.68 dB; SSIM: 0.7846; PSNRB: 0.00 dB;PSNR_Y: 29.04 dB; SSIM_Y: 0.8054; PSNRB_Y: 0.00 dB.\n",
            "Testing 57 imgname              - PSNR: 27.59 dB; SSIM: 0.7856; PSNRB: 0.00 dB;PSNR_Y: 28.95 dB; SSIM_Y: 0.8064; PSNRB_Y: 0.00 dB.\n",
            "Testing 58 imgname              - PSNR: 27.60 dB; SSIM: 0.7824; PSNRB: 0.00 dB;PSNR_Y: 28.96 dB; SSIM_Y: 0.8037; PSNRB_Y: 0.00 dB.\n",
            "Testing 59 imgname              - PSNR: 27.64 dB; SSIM: 0.7737; PSNRB: 0.00 dB;PSNR_Y: 28.99 dB; SSIM_Y: 0.7957; PSNRB_Y: 0.00 dB.\n",
            "Testing 60 imgname              - PSNR: 27.58 dB; SSIM: 0.7686; PSNRB: 0.00 dB;PSNR_Y: 28.94 dB; SSIM_Y: 0.7912; PSNRB_Y: 0.00 dB.\n",
            "Testing 61 imgname              - PSNR: 27.48 dB; SSIM: 0.7681; PSNRB: 0.00 dB;PSNR_Y: 28.84 dB; SSIM_Y: 0.7902; PSNRB_Y: 0.00 dB.\n",
            "Testing 62 imgname              - PSNR: 27.41 dB; SSIM: 0.7725; PSNRB: 0.00 dB;PSNR_Y: 28.77 dB; SSIM_Y: 0.7940; PSNRB_Y: 0.00 dB.\n",
            "Testing 63 imgname              - PSNR: 27.48 dB; SSIM: 0.7803; PSNRB: 0.00 dB;PSNR_Y: 28.84 dB; SSIM_Y: 0.8014; PSNRB_Y: 0.00 dB.\n",
            "Testing 64 imgname              - PSNR: 27.35 dB; SSIM: 0.7779; PSNRB: 0.00 dB;PSNR_Y: 28.70 dB; SSIM_Y: 0.7989; PSNRB_Y: 0.00 dB.\n",
            "Testing 65 imgname              - PSNR: 27.39 dB; SSIM: 0.7748; PSNRB: 0.00 dB;PSNR_Y: 28.74 dB; SSIM_Y: 0.7961; PSNRB_Y: 0.00 dB.\n",
            "Testing 66 imgname              - PSNR: 27.34 dB; SSIM: 0.7694; PSNRB: 0.00 dB;PSNR_Y: 28.70 dB; SSIM_Y: 0.7914; PSNRB_Y: 0.00 dB.\n",
            "Testing 67 imgname              - PSNR: 27.52 dB; SSIM: 0.7774; PSNRB: 0.00 dB;PSNR_Y: 28.88 dB; SSIM_Y: 0.7989; PSNRB_Y: 0.00 dB.\n",
            "Testing 68 imgname              - PSNR: 27.63 dB; SSIM: 0.7863; PSNRB: 0.00 dB;PSNR_Y: 28.99 dB; SSIM_Y: 0.8069; PSNRB_Y: 0.00 dB.\n",
            "Testing 69 imgname              - PSNR: 27.63 dB; SSIM: 0.7881; PSNRB: 0.00 dB;PSNR_Y: 28.99 dB; SSIM_Y: 0.8087; PSNRB_Y: 0.00 dB.\n",
            "Testing 70 imgname              - PSNR: 27.61 dB; SSIM: 0.7845; PSNRB: 0.00 dB;PSNR_Y: 28.97 dB; SSIM_Y: 0.8053; PSNRB_Y: 0.00 dB.\n",
            "Testing 71 imgname              - PSNR: 27.57 dB; SSIM: 0.7758; PSNRB: 0.00 dB;PSNR_Y: 28.93 dB; SSIM_Y: 0.7973; PSNRB_Y: 0.00 dB.\n",
            "Testing 72 imgname              - PSNR: 27.52 dB; SSIM: 0.7707; PSNRB: 0.00 dB;PSNR_Y: 28.88 dB; SSIM_Y: 0.7928; PSNRB_Y: 0.00 dB.\n",
            "Testing 73 imgname              - PSNR: 27.29 dB; SSIM: 0.7683; PSNRB: 0.00 dB;PSNR_Y: 28.64 dB; SSIM_Y: 0.7898; PSNRB_Y: 0.00 dB.\n",
            "Testing 74 imgname              - PSNR: 27.23 dB; SSIM: 0.7641; PSNRB: 0.00 dB;PSNR_Y: 28.59 dB; SSIM_Y: 0.7858; PSNRB_Y: 0.00 dB.\n",
            "Testing 75 imgname              - PSNR: 27.39 dB; SSIM: 0.7662; PSNRB: 0.00 dB;PSNR_Y: 28.75 dB; SSIM_Y: 0.7878; PSNRB_Y: 0.00 dB.\n",
            "Testing 76 imgname              - PSNR: 27.58 dB; SSIM: 0.7731; PSNRB: 0.00 dB;PSNR_Y: 28.95 dB; SSIM_Y: 0.7944; PSNRB_Y: 0.00 dB.\n",
            "Testing 77 imgname              - PSNR: 27.51 dB; SSIM: 0.7756; PSNRB: 0.00 dB;PSNR_Y: 28.87 dB; SSIM_Y: 0.7963; PSNRB_Y: 0.00 dB.\n",
            "Testing 78 imgname              - PSNR: 27.44 dB; SSIM: 0.7655; PSNRB: 0.00 dB;PSNR_Y: 28.81 dB; SSIM_Y: 0.7876; PSNRB_Y: 0.00 dB.\n",
            "Testing 79 imgname              - PSNR: 27.51 dB; SSIM: 0.7674; PSNRB: 0.00 dB;PSNR_Y: 28.88 dB; SSIM_Y: 0.7892; PSNRB_Y: 0.00 dB.\n",
            "Testing 80 imgname              - PSNR: 27.78 dB; SSIM: 0.7775; PSNRB: 0.00 dB;PSNR_Y: 29.15 dB; SSIM_Y: 0.7988; PSNRB_Y: 0.00 dB.\n",
            "Testing 81 imgname              - PSNR: 28.04 dB; SSIM: 0.7858; PSNRB: 0.00 dB;PSNR_Y: 29.40 dB; SSIM_Y: 0.8066; PSNRB_Y: 0.00 dB.\n",
            "Testing 82 imgname              - PSNR: 28.20 dB; SSIM: 0.7930; PSNRB: 0.00 dB;PSNR_Y: 29.56 dB; SSIM_Y: 0.8135; PSNRB_Y: 0.00 dB.\n",
            "Testing 83 imgname              - PSNR: 28.38 dB; SSIM: 0.8017; PSNRB: 0.00 dB;PSNR_Y: 29.76 dB; SSIM_Y: 0.8219; PSNRB_Y: 0.00 dB.\n",
            "Testing 84 imgname              - PSNR: 28.63 dB; SSIM: 0.8124; PSNRB: 0.00 dB;PSNR_Y: 30.00 dB; SSIM_Y: 0.8316; PSNRB_Y: 0.00 dB.\n",
            "Testing 85 imgname              - PSNR: 28.64 dB; SSIM: 0.8144; PSNRB: 0.00 dB;PSNR_Y: 30.01 dB; SSIM_Y: 0.8337; PSNRB_Y: 0.00 dB.\n",
            "Testing 86 imgname              - PSNR: 28.67 dB; SSIM: 0.8164; PSNRB: 0.00 dB;PSNR_Y: 30.05 dB; SSIM_Y: 0.8357; PSNRB_Y: 0.00 dB.\n",
            "Testing 87 imgname              - PSNR: 28.67 dB; SSIM: 0.8173; PSNRB: 0.00 dB;PSNR_Y: 30.05 dB; SSIM_Y: 0.8366; PSNRB_Y: 0.00 dB.\n",
            "Testing 88 imgname              - PSNR: 28.78 dB; SSIM: 0.8207; PSNRB: 0.00 dB;PSNR_Y: 30.15 dB; SSIM_Y: 0.8398; PSNRB_Y: 0.00 dB.\n",
            "Testing 89 imgname              - PSNR: 28.85 dB; SSIM: 0.8258; PSNRB: 0.00 dB;PSNR_Y: 30.23 dB; SSIM_Y: 0.8445; PSNRB_Y: 0.00 dB.\n",
            "Testing 90 imgname              - PSNR: 29.09 dB; SSIM: 0.8342; PSNRB: 0.00 dB;PSNR_Y: 30.47 dB; SSIM_Y: 0.8525; PSNRB_Y: 0.00 dB.\n",
            "Testing 91 imgname              - PSNR: 29.13 dB; SSIM: 0.8369; PSNRB: 0.00 dB;PSNR_Y: 30.51 dB; SSIM_Y: 0.8550; PSNRB_Y: 0.00 dB.\n",
            "Testing 92 imgname              - PSNR: 29.14 dB; SSIM: 0.8378; PSNRB: 0.00 dB;PSNR_Y: 30.52 dB; SSIM_Y: 0.8557; PSNRB_Y: 0.00 dB.\n",
            "Testing 93 imgname              - PSNR: 29.24 dB; SSIM: 0.8418; PSNRB: 0.00 dB;PSNR_Y: 30.62 dB; SSIM_Y: 0.8594; PSNRB_Y: 0.00 dB.\n",
            "Testing 94 imgname              - PSNR: 29.56 dB; SSIM: 0.8541; PSNRB: 0.00 dB;PSNR_Y: 30.95 dB; SSIM_Y: 0.8713; PSNRB_Y: 0.00 dB.\n",
            "Testing 95 imgname              - PSNR: 30.04 dB; SSIM: 0.8642; PSNRB: 0.00 dB;PSNR_Y: 31.43 dB; SSIM_Y: 0.8809; PSNRB_Y: 0.00 dB.\n",
            "Testing 96 imgname              - PSNR: 29.94 dB; SSIM: 0.8601; PSNRB: 0.00 dB;PSNR_Y: 31.34 dB; SSIM_Y: 0.8774; PSNRB_Y: 0.00 dB.\n",
            "Testing 97 imgname              - PSNR: 29.70 dB; SSIM: 0.8536; PSNRB: 0.00 dB;PSNR_Y: 31.08 dB; SSIM_Y: 0.8713; PSNRB_Y: 0.00 dB.\n",
            "Testing 98 imgname              - PSNR: 29.83 dB; SSIM: 0.8519; PSNRB: 0.00 dB;PSNR_Y: 31.22 dB; SSIM_Y: 0.8698; PSNRB_Y: 0.00 dB.\n",
            "Testing 99 imgname              - PSNR: 29.98 dB; SSIM: 0.8496; PSNRB: 0.00 dB;PSNR_Y: 31.37 dB; SSIM_Y: 0.8676; PSNRB_Y: 0.00 dB.\n",
            "\n",
            "-- Average PSNR/SSIM(RGB): 27.77 dB; 0.7719\n",
            "-- Average PSNR_Y/SSIM_Y: 29.13 dB; 0.7939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# frames = [cv2.cvtColor(np.array(frame), cv2.COLOR_BGR2RGB) for frame in frames]\n",
        "vanilla_sr_video_new = np.array(vanilla_sr_video)\n",
        "\n",
        "_, height, width, layers = vanilla_sr_video_new.shape\n",
        "\n",
        "video = cv2.VideoWriter(\"vanilla_sr_new.avi\", 0, 24, (width,height))\n",
        "\n",
        "for image in vanilla_sr_video_new:\n",
        "    video.write(image)\n",
        "video.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "C1YryscrVkSO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's actually make dataloaders\n",
        "BATCH_SIZE = 2\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "train_blurry_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=blurry_train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_blurry_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=blurry_val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "pCINpKFCAB2L"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_blurry_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcqPmlggHgYE",
        "outputId": "bc85614d-7cf3-48a3-9cf9-96502892c351"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Structure"
      ],
      "metadata": {
        "id": "0A5ktkHi7TmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyVul03YTVDQ",
        "outputId": "b4c03450-b006-4ce6-b9ff-b97f73aa444b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model for improved video superresolution\n",
        "import math\n",
        "class TrNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_size=5,\n",
        "        filter1_size=5,\n",
        "        filter2_size=16,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=3,\n",
        "            out_channels=filter1_size,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=math.floor(kernel_size / 2)\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=filter1_size,\n",
        "            out_channels=3,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=math.floor(kernel_size / 2)\n",
        "        )\n",
        "        # self.fc1 = nn.Linear(in_features=filter2_size * kernel_size ** 2, out_features=fc1_size)\n",
        "        # self.fc2 = nn.Linear(in_features=fc1_size, out_features=fc2_size)\n",
        "        # self.fc3 = nn.Linear(in_features=image_size + fc2_size, out_features=image_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # assume x is (frames, color channels, height, width) tensor\n",
        "        out = torch.diff(x, dim=0, prepend=torch.unsqueeze(x[0], 0))\n",
        "\n",
        "        out = F.relu(self.conv1(out))\n",
        "        out = F.relu(self.conv2(out))\n",
        "        return out\n",
        "\n",
        "custom_model = TrNet().to(device)"
      ],
      "metadata": {
        "id": "2HENYtEQVk3x"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the input\n",
        "image1 = frames\n",
        "print(image1.shape)\n",
        "print()\n",
        "image1 = image1.to(device)\n",
        "custom_model(image1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iAza--_Suk5",
        "outputId": "6c0a7c88-7eea-4c90-9991-63d85c2b1100"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 3, 180, 320])\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 3, 180, 320])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vanilla_sr_video = []\n",
        "lr_video = blurry_train_dataset[3][0] # 3rd data point in the blurry training version\n",
        "lr_video = get_video_output(lr_video)\n",
        "# lr_video = [cv2.cvtColor(np.array(frame), cv2.COLOR_BGR2RGB) for frame in lr_video]\n",
        "gt_video = sharp_train_dataset[3][0] # 3rd data point in the sharp training version\n",
        "gt_video = get_video_output(gt_video)\n",
        "# gt_video = [cv2.cvtColor(np.array(frame), cv2.COLOR_BGR2RGB) for frame in gt_video]\n",
        "\n",
        "lr_video = [frame / 255 for frame in lr_video]\n",
        "gt_video = [frame / 255 for frame in gt_video]\n",
        "\n",
        "for idx, (img_lq, img_gt) in enumerate(zip(lr_video, gt_video)):\n",
        "    # read image\n",
        "    img_lq = np.transpose(img_lq if img_lq.shape[2] == 1 else img_lq[:, :, [2, 1, 0]], (2, 0, 1))  # HCW-BGR to CHW-RGB\n",
        "    img_lq = torch.from_numpy(img_lq).float().unsqueeze(0).to(device)  # CHW-RGB to NCHW-RGB\n",
        "\n",
        "    # inference\n",
        "    with torch.no_grad():\n",
        "        # pad input image to be a multiple of window_size\n",
        "        _, _, h_old, w_old = img_lq.size()\n",
        "        h_pad = (h_old // WINDOW_SIZE + 1) * WINDOW_SIZE - h_old\n",
        "        w_pad = (w_old // WINDOW_SIZE + 1) * WINDOW_SIZE - w_old\n",
        "        img_lq = torch.cat([img_lq, torch.flip(img_lq, [2])], 2)[:, :, :h_old + h_pad, :]\n",
        "        img_lq = torch.cat([img_lq, torch.flip(img_lq, [3])], 3)[:, :, :, :w_old + w_pad]\n",
        "        output = model(img_lq)\n",
        "        output = output[..., :h_old * SCALE_FACTOR, :w_old * SCALE_FACTOR]\n",
        "\n",
        "    # save image\n",
        "    output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
        "    if output.ndim == 3:\n",
        "        output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))  # CHW-RGB to HCW-BGR\n",
        "    output = (output * 255.0).round().astype(np.uint8)  # float32 to uint8\n",
        "    \n",
        "    # ----------------------------------------------------\n",
        "    # appending to later make a video\n",
        "    vanilla_sr_video.append(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM7Ir3gEjI-x",
        "outputId": "b5df12ca-557e-4069-a92d-00656227307f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 3, 180, 320])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_sr_model = model"
      ],
      "metadata": {
        "id": "uGyAT3sFsYtI"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, gt_data, model, optimizer, epoch):\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "\n",
        "    batches = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "    batches.set_description(\"Epoch NA: Loss (NA) Accuracy (NA %)\")\n",
        "    for batch_idx, (data, target) in batches:\n",
        "        video = torch.tensor([]).to(device)\n",
        "        # Move data to appropriate device\n",
        "        moved_data = data.to(device)\n",
        "        moved_data = get_video_output(moved_data) / 255\n",
        "\n",
        "        target = gt_data[target][0]\n",
        "        target = target.to(device)\n",
        "        target = get_video_output(target) / 255\n",
        "\n",
        "        for idx, (img_lq, img_gt) in enumerate(zip(moved_data, target)):\n",
        "            # read image\n",
        "            img_lq = np.transpose(img_lq if img_lq.shape[2] == 1 else img_lq[:, :, [2, 1, 0]], (2, 0, 1))  # HCW-BGR to CHW-RGB\n",
        "            img_lq = torch.from_numpy(img_lq).float().unsqueeze(0).to(device)  # CHW-RGB to NCHW-RGB\n",
        "\n",
        "            # inference\n",
        "            with torch.no_grad():\n",
        "                # pad input image to be a multiple of window_size\n",
        "                _, _, h_old, w_old = img_lq.size()\n",
        "                h_pad = (h_old // WINDOW_SIZE + 1) * WINDOW_SIZE - h_old\n",
        "                w_pad = (w_old // WINDOW_SIZE + 1) * WINDOW_SIZE - w_old\n",
        "                img_lq = torch.cat([img_lq, torch.flip(img_lq, [2])], 2)[:, :, :h_old + h_pad, :]\n",
        "                img_lq = torch.cat([img_lq, torch.flip(img_lq, [3])], 3)[:, :, :, :w_old + w_pad]\n",
        "                output = model(img_lq)\n",
        "                output = output[..., :h_old * SCALE_FACTOR, :w_old * SCALE_FACTOR]\n",
        "\n",
        "            # save image\n",
        "            output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
        "            if output.ndim == 3:\n",
        "                output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))  # CHW-RGB to HCW-BGR\n",
        "            \n",
        "            # ----------------------------------------------------\n",
        "            # appending to later make a video\n",
        "            video.append(output)\n",
        "\n",
        "        # Zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Compute forward pass, loss, and gradients\n",
        "        transitions = model(moved_data)\n",
        "        loss = F.mse_loss(video + transitions, target)\n",
        "        loss.backward()\n",
        "        train_loss.append(loss)\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        # Compute and record accuracy\n",
        "\n",
        "        batches.set_description(\n",
        "            \"Epoch {:d}: Loss ({:.2e})\".format(\n",
        "                epoch, loss.item()\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return train_loss"
      ],
      "metadata": {
        "id": "YL9EJdp-hPXj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = OrderedDict()\n",
        "test_results['psnr'] = []\n",
        "test_results['ssim'] = []\n",
        "test_results['psnr_y'] = []\n",
        "test_results['ssim_y'] = []\n",
        "test_results['psnrb'] = []\n",
        "test_results['psnrb_y'] = []\n",
        "psnr, ssim, psnr_y, ssim_y, psnrb, psnrb_y = 0, 0, 0, 0, 0, 0\n",
        "\n",
        "vanilla_sr_video = []\n",
        "lr_video = blurry_train_dataset[3][0] # 3rd data point in the blurry training version\n",
        "lr_video = get_video_output(lr_video)\n",
        "# lr_video = [cv2.cvtColor(np.array(frame), cv2.COLOR_BGR2RGB) for frame in lr_video]\n",
        "gt_video = sharp_train_dataset[3][0] # 3rd data point in the sharp training version\n",
        "gt_video = get_video_output(gt_video)\n",
        "# gt_video = [cv2.cvtColor(np.array(frame), cv2.COLOR_BGR2RGB) for frame in gt_video]\n",
        "\n",
        "lr_video = [frame / 255 for frame in lr_video]\n",
        "gt_video = [frame / 255 for frame in gt_video]\n",
        "\n",
        "for idx, (img_lq, img_gt) in enumerate(zip(lr_video, gt_video)):\n",
        "    # read image\n",
        "    img_lq = np.transpose(img_lq if img_lq.shape[2] == 1 else img_lq[:, :, [2, 1, 0]], (2, 0, 1))  # HCW-BGR to CHW-RGB\n",
        "    img_lq = torch.from_numpy(img_lq).float().unsqueeze(0).to(device)  # CHW-RGB to NCHW-RGB\n",
        "\n",
        "    # inference\n",
        "    with torch.no_grad():\n",
        "        # pad input image to be a multiple of window_size\n",
        "        _, _, h_old, w_old = img_lq.size()\n",
        "        h_pad = (h_old // WINDOW_SIZE + 1) * WINDOW_SIZE - h_old\n",
        "        w_pad = (w_old // WINDOW_SIZE + 1) * WINDOW_SIZE - w_old\n",
        "        img_lq = torch.cat([img_lq, torch.flip(img_lq, [2])], 2)[:, :, :h_old + h_pad, :]\n",
        "        img_lq = torch.cat([img_lq, torch.flip(img_lq, [3])], 3)[:, :, :, :w_old + w_pad]\n",
        "        output = model(img_lq)\n",
        "        output = output[..., :h_old * SCALE_FACTOR, :w_old * SCALE_FACTOR]\n",
        "\n",
        "    # save image\n",
        "    output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
        "    if output.ndim == 3:\n",
        "        output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))  # CHW-RGB to HCW-BGR\n",
        "    output = (output * 255.0).round().astype(np.uint8)  # float32 to uint8\n",
        "    \n",
        "    # ----------------------------------------------------\n",
        "    # appending to later make a video\n",
        "    vanilla_sr_video.append(output)\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "\n",
        "    # evaluate psnr/ssim/psnr_b\n",
        "    if img_gt is not None:\n",
        "        img_gt = (img_gt * 255.0).round().astype(np.uint8)  # float32 to uint8\n",
        "        img_gt = img_gt[:h_old * SCALE_FACTOR, :w_old * SCALE_FACTOR, ...]  # crop gt\n",
        "        img_gt = np.squeeze(img_gt)\n",
        "\n",
        "        psnr = util.calculate_psnr(output, img_gt, crop_border=BORDER)\n",
        "        ssim = util.calculate_ssim(output, img_gt, crop_border=BORDER)\n",
        "        test_results['psnr'].append(psnr)\n",
        "        test_results['ssim'].append(ssim)\n",
        "        if img_gt.ndim == 3:  # RGB image\n",
        "            psnr_y = util.calculate_psnr(output, img_gt, crop_border=BORDER, test_y_channel=True)\n",
        "            ssim_y = util.calculate_ssim(output, img_gt, crop_border=BORDER, test_y_channel=True)\n",
        "            test_results['psnr_y'].append(psnr_y)\n",
        "            test_results['ssim_y'].append(ssim_y)\n",
        "        if ARG_TASK in ['jpeg_car', 'color_jpeg_car']:\n",
        "            psnrb = util.calculate_psnrb(output, img_gt, crop_border=BORDER, test_y_channel=False)\n",
        "            test_results['psnrb'].append(psnrb)\n",
        "            if ARG_TASK in ['color_jpeg_car']:\n",
        "                psnrb_y = util.calculate_psnrb(output, img_gt, crop_border=BORDER, test_y_channel=True)\n",
        "                test_results['psnrb_y'].append(psnrb_y)\n",
        "        print('Testing {:d} {:20s} - PSNR: {:.2f} dB; SSIM: {:.4f}; PSNRB: {:.2f} dB;'\n",
        "                'PSNR_Y: {:.2f} dB; SSIM_Y: {:.4f}; PSNRB_Y: {:.2f} dB.'.\n",
        "                format(idx, \"imgname\", psnr, ssim, psnrb, psnr_y, ssim_y, psnrb_y))\n",
        "    else:\n",
        "        print('Testing {:d} {:20s}'.format(idx, \"imgname\"))"
      ],
      "metadata": {
        "id": "0EngYbvAyS3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate(dataloader, gt_data, model):\n",
        "    test_results = OrderedDict()\n",
        "    test_results['psnr'] = []\n",
        "    test_results['ssim'] = []\n",
        "    test_results['psnr_y'] = []\n",
        "    test_results['ssim_y'] = []\n",
        "    test_results['psnrb'] = []\n",
        "    test_results['psnrb_y'] = []\n",
        "    psnr, ssim, psnr_y, ssim_y, psnrb, psnrb_y = 0, 0, 0, 0, 0, 0\n",
        "    model.eval()\n",
        "\n",
        "    for video, video_idx in dataloader:\n",
        "        gt_video = gt_data[video_idx][0].to(device)\n",
        "        video = video.to(device)\n",
        "\n",
        "        gt_video = get_video_output(gt_video) / 255\n",
        "        video = get_video_output(video) / 255\n",
        "\n",
        "        for idx, (img_lq, img_gt) in enumerate(zip(video, gt_video)):\n",
        "            # read image\n",
        "            img_lq = np.transpose(img_lq if img_lq.shape[2] == 1 else img_lq[:, :, [2, 1, 0]], (2, 0, 1))  # HCW-BGR to CHW-RGB\n",
        "            img_lq = torch.from_numpy(img_lq).float().unsqueeze(0).to(device)  # CHW-RGB to NCHW-RGB\n",
        "\n",
        "            # inference\n",
        "            with torch.no_grad():\n",
        "                # pad input image to be a multiple of window_size\n",
        "                _, _, h_old, w_old = img_lq.size()\n",
        "                h_pad = (h_old // WINDOW_SIZE + 1) * WINDOW_SIZE - h_old\n",
        "                w_pad = (w_old // WINDOW_SIZE + 1) * WINDOW_SIZE - w_old\n",
        "                img_lq = torch.cat([img_lq, torch.flip(img_lq, [2])], 2)[:, :, :h_old + h_pad, :]\n",
        "                img_lq = torch.cat([img_lq, torch.flip(img_lq, [3])], 3)[:, :, :, :w_old + w_pad]\n",
        "                output = model(img_lq)\n",
        "                output = output[..., :h_old * SCALE_FACTOR, :w_old * SCALE_FACTOR]\n",
        "\n",
        "            # save image\n",
        "            output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
        "            if output.ndim == 3:\n",
        "                output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))  # CHW-RGB to HCW-BGR\n",
        "            output = (output * 255.0).round().astype(np.uint8)  # float32 to uint8\n",
        "\n",
        "            # evaluate psnr/ssim/psnr_b\n",
        "            if img_gt is not None:\n",
        "                img_gt = (img_gt * 255.0).round().astype(np.uint8)  # float32 to uint8\n",
        "                img_gt = img_gt[:h_old * SCALE_FACTOR, :w_old * SCALE_FACTOR, ...]  # crop gt\n",
        "                img_gt = np.squeeze(img_gt)\n",
        "\n",
        "                psnr = util.calculate_psnr(output, img_gt, crop_border=BORDER)\n",
        "                ssim = util.calculate_ssim(output, img_gt, crop_border=BORDER)\n",
        "                test_results['psnr'].append(psnr)\n",
        "                test_results['ssim'].append(ssim)\n",
        "                if img_gt.ndim == 3:  # RGB image\n",
        "                    psnr_y = util.calculate_psnr(output, img_gt, crop_border=BORDER, test_y_channel=True)\n",
        "                    ssim_y = util.calculate_ssim(output, img_gt, crop_border=BORDER, test_y_channel=True)\n",
        "                    test_results['psnr_y'].append(psnr_y)\n",
        "                    test_results['ssim_y'].append(ssim_y)\n",
        "                if ARG_TASK in ['jpeg_car', 'color_jpeg_car']:\n",
        "                    psnrb = util.calculate_psnrb(output, img_gt, crop_border=BORDER, test_y_channel=False)\n",
        "                    test_results['psnrb'].append(psnrb)\n",
        "                    if ARG_TASK in ['color_jpeg_car']:\n",
        "                        psnrb_y = util.calculate_psnrb(output, img_gt, crop_border=BORDER, test_y_channel=True)\n",
        "                        test_results['psnrb_y'].append(psnrb_y)\n",
        "                print('Testing {:d} {:20s} - PSNR: {:.2f} dB; SSIM: {:.4f}; PSNRB: {:.2f} dB;'\n",
        "                        'PSNR_Y: {:.2f} dB; SSIM_Y: {:.4f}; PSNRB_Y: {:.2f} dB.'.\n",
        "                        format(idx, \"imgname\", psnr, ssim, psnrb, psnr_y, ssim_y, psnrb_y))\n",
        "            else:\n",
        "                print('Testing {:d} {:20s}'.format(idx, \"imgname\"))\n",
        "    ave_psnr = sum(test_results['psnr']) / len(test_results['psnr'])\n",
        "    ave_ssim = sum(test_results['ssim']) / len(test_results['ssim'])\n",
        "    return ave_psnr, ave_ssim\n",
        "\n",
        "            \n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "FBMyxpowwc-3"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "lr = 0.001\n",
        "\n",
        "def create_optimizer(net, lr):\n",
        "    # TODO: Create optimizer\n",
        "    return torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "optim = create_optimizer(custom_model, lr)\n",
        "train_loader = train_blurry_dataloader\n",
        "val_loader = val_blurry_dataloader\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch: {}\\tValidation Accuracy: {:.4f}%'.format(epoch, evaluate(val_loader, sharp_val_dataset, custom_model) * 100))\n",
        "    train(train_loader, sharp_train_dataset, custom_model, optim, epoch)"
      ],
      "metadata": {
        "id": "qSH41PtjS-oT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}